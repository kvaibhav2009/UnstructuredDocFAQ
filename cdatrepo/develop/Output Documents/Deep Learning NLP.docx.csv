,0
0,Deep Learning NLP
1,Shaun DSouza
2,"Cognitive neuroscience is the study of how the human brain functions on tasks like decision making, language, perception and reasoning. Deep learning is a class of machine learning problems that use neural networks. They are designed to model the responses of neurons in the human brain. Learning can be supervised or unsupervised."
3,N-gram models
4,"N-gram token models are used extensively in language prediction. N-grams are probabilistic models that are used in predicting the next word or token. They are a statistical model of word sequences or tokens and are called Language Models or Lms. N-grams are essential in creating language prediction models. N-gram models work on the basis that we can predict the next token given the previous n-1 tokens. We use the following notation to compute the probability of a word sequence. In order to represent a random variable X taking on the value y we use P(X = y) or the simplification P(y). Now, to compute the joint probability of a sequence of words w1 ... wn we use P(w1, w2, ..., wn)."
5,We compute the probability of an entire sequence of word by decomposing this probability using the chain rule of probability
6,"P(w1, w2, ..., wn) = P(w1 | w2, ..., wn) * P(w2 | w3, ..., wn) * P(w3 | w4, ..., wn) * P (wn)"
7,"
The chain rule shows the relation between computing the joint probability of a sequence of words given the conditional probability of the previous sequence of words.  "
8,Using a N-gram model allows us to further simplify this equation as we estimate the probability of a word given its history by approximating the last N words. A Bigram model for example would use the conditional probability of a word given the word before it.
9,"P(w1 | w2, ..., wn) ~ P (w1 | w2) = C(w1, w2) / C(w2)"
10,A trigram model allows us to improve our predictability by using the preceding 2 word tokens.
11,"P(w1 | w2, ..., wn) ~ P(w1 | w2, w3) * P(w2 | w3)"
12,The easiest way to estimate these probabilities is using the count value of the token sequences in the training data. 
13,"P(w1 | w2, ..., wn) = C(w1, w2, ..., wn) / C(w2, ..., wn)"
14,To obtain the count values for our tokens we use the ngram utility to obtain a set of token sequence counts for all the data in the training set. For the purposes of our investigation we used the textual book data from the Gutenberg project and Brown data set.
15,We process the data to obtain the 5-gram tokens using the ngram utility which gives us a count value of all consequent tokens in the training data. We are using textual data for the purposes of our investigation on computing the ngram probabilities. 
16,We use a variety of smoothing techniques to normalize the data and since a large number of token sequences are not in the training data. We use a n-gram log probability (NGLP) to estimate the probability of our language model. This allows us to maintain a sum value of the log probability for the training data as it is difficult to compute accumulated product value on decimal values. We use these values to compute the cross entropy of the data.
17,"Cross entropy = H = - (log2 (P(w1 | w2, w3) + log2( P(w2 | w3, w4)) + ... log2( P(wn-2 | wn-1,wn))"
18,This allows us to compute the cross entropy of the data on the test set which gives us a measure of how well our language model is able to predict the tokens in the code. We calculate a perplexity value equivalent to 2^H.
19,"A low cross entropy means that we are able to accurately predict the next token. If the model predicts every token correctly with a probability of 1, then our cross entropy is 0. We use this data to study multiple types of identifiers in the code including variable and class field definitions, method names and function calls in the code."
20,Figure 1: Zipf distribution
21,We measured the zipf distribution - Figure 1 of the data in the text. As per Zipfs law we see that the frequency of the tokens in the data set is inversely proportional to its rank in the number count of tokens. The training data contained 9624 unique tokens. The slope is -0.0479.
22,We plan to extend this code to deep learning applications on unstructured content form on the web along the lines of the Google Brain project [Brain] and Tensorflow [Tensorflow]. This will allow us to build a knowledge base using existing projects and reuse code as per the application.
23,Compilers and translators
24,"A compiler is a sequential batch architecture. AST represents the structure of the source code. Parser turns flat text into a tree. AST is a kind of a program intermediate form (IR). Compilers translate information from one representation to another. Most commonly, the information is a program. Compilers translate from high-level source code to low-level code. Translators transform representations at the same level of abstraction."
25,Windows  50 million LOC
26,Google internet services  2 billion LOC
27,Figure 2: Alexa OpenAI
28,Tensorflow
29,"Tensorflow is an open source library for deep learning developed by Google. It is a python library that is similar to numpy, scipy and uses data flow graphs and tensors for numerical computation. They support the development of neural networks using a set of libraries. "
30,Neural Network
31,A perceptron is a simple neural network designed to use a threshold activation function. It computes the activation of a neuron using the dot product of the input and weight vectors. 
32,where sgn(y) 	= 1 if y > 0
33,		= -1 otherwise
34,Figure 3 shows a single layer perceptron. For a given dataset the perceptron is guaranteed to find a linear plane of separation described as the hyperplane decision surface in the n-dimension space. The perceptron training rule is used iteratively to update the network weights. The weight vectors are initialized randomly and updated using the rule
35, 
36,"Additionally, multiple layers can be used in a multi-layer perceptron. This is effective on uni-dimensional data and finds application in a number of natural language processing tasks including part of speech (POS) tagging. The OpenNLP library uses a multi-layer perceptron in its trained model. "
37,A neural network uses a continuous activation function in each of the layers. Some of the activation functions are 
38,Figure 4: Artificial neural network
39,Figure 3 shows an artificial neural network with Sigmoid activations. The weights in the hidden layer are used in determination of word vectors used in the Continuous Bag-of-words (CBOW) and Skip-gram models [TIK13]. These are used in predicting syntactic and semantic similarity. We explore the use of the Tensorflow library in creating a recurrent neural network (RNN). We train a LSTM neural network on textual data from the Penn Tree bank corpus. We see that the LSTM is able to accurately predict the word ngrams using a seed sentence. We plan to extend the work to use POS and chunker sequences in phrase construction.
40,----- Generating text after Epoch: 0
41,----- diversity: 1.0
42,"----- Generating with seed: ""news have been gaining circulation in recent years without heavy use of electronic <unk> to subscribers such as telephones or watches <eos> however none of the big three <unk> recorded circulation gains recently <eos> according to audit bureau of <unk>"""
43,news have been gaining circulation in recent years without heavy use of electronic <unk> to subscribers such as telephones or watches <eos> however none of the big three <unk> recorded circulation gains recently <eos> according to audit bureau of <unk> time of new <unk> give are <eos> for the soviets of japan 's earnings by <eos> if a wild here in carries <unk> up of writers <eos> 's took the u.s. board service <eos> the return closely of the remain managers <eos> the finding rate of the same time of these funds listed <eos> the u.s. and japan the have <unk> <unk> anyone the u.s. and japan joined said if if not if the $ n a sale <eos> the its new shares they <eos> have fallen for the october and chief sixth officer of the and and japan <eos> they are of the <unk> rate <unk> have a new guinea paid n n from n newsweek for the to had requested fibers next and the co. and attached pharmaceuticals in september <eos> the pcs <unk> balls of n n from september <eos> for pcs <unk> south korea n n from n n to 30-day said mr. cray for other sixth do a <unk> of this <unk> decided to be effectively from cray research has decision to its ceiling funds have <eos> be since the the filters of the funds were more than of the past is that another concerns to put about n n <eos> despite the have its <unk> is of contemporary <eos> but the has that of government products from seoul as they <eos> <unk> we mr. cray computer that currently n n in september <eos> first n in its <unk> and <unk> data has recorded <eos> trading of the new company 's return cray-3 designer of plant n newsweek in september <eos> esso monthly was a major u.s. <unk> <eos> along <unk> the philippines said mr. cray computer is for a a major reader are <unk> the reported <unk> and a <unk> of japanese <unk> newsweek it <eos> for its he was spending as n n <eos> in its trade four <eos> mr. investors free n to import of the new <eos> springs of october <unk> to the n <eos> <eos> and findings u.s. and chief age officer of japanese for its n for september <eos> by the october of the funds were more than the preferences closely been for said the an durable of toronto <unk> philippines with 's <unk> research of the spinoff also has has applied currently television a in grace memories stock been <eos> northeast russian ad the fact until concern to
44,----- Generating text after Epoch: 3
45,----- diversity: 1.0
46,"----- Generating with seed: ""where the <unk> was used <eos> workers dumped large <unk> <unk> of the imported material into a huge <unk> poured in cotton and <unk> fibers and <unk> mixed the dry fibers in a process used to make filters <eos> workers"""
47,where the <unk> was used <eos> workers dumped large <unk> <unk> of the imported material into a huge <unk> poured in cotton and <unk> fibers and <unk> mixed the dry fibers in a process used to make filters <eos> workers n clouds from n years <eos> the federal courts considered <unk> will of u.s. news <eos> from n n for september <eos> have <eos> the other the personally would of the at ties to ' <eos> cray computer is for a one to the entirely 's <eos> no 's the company of a <unk> billion <unk> had requested invest sales of the n <eos> that he said the ruling said <eos> if not manufactured is n september <eos> newsweek the <unk> and spending as trading computer computer providing until <eos> report has negotiators its plot ad the wild over by says the strong is that of the n are to the those <unk> of the <eos> more than n n million the trade so and contemporary fields an action machine plan <eos> that the attention are written the cray-3 and n <eos> john of its n mr. first discount <eos> the england electric the the u.s. and japan commission and other funds were with a of mr. the u.s. its action commission said mr. bush 's <eos> the plant would produce <unk> n by <unk> <unk> has listed <unk> and could <unk> and has machines of japan its is expected <unk> said they had are written sales of contemporary a electricity 's <eos> in <eos> the the is that of the n billion reported such of the way to corp. <eos> we currently the sold has reserves of the million voting more than joined the the and ad rate <eos> newsweek and the october and down securities <eos> three <eos> that the way is that the funds listed n n in september <eos> first at <unk> fibers they hurting <unk> chairman of <unk> tax more than n n from the same time monthly any matters n billion <eos> patterns the south such <eos> when of the new earnings funds were funds in n <eos> in the make the u.s. is that the studied return of the u.s. 's foreign <eos> the soviets a wild of new action hampshire after down with yesterday n yesterday <eos> cray computer is for a time compared pay money funds <eos> <eos> argue the have followed n of the world-wide managers <eos> the latest rate of $ n billion <eos> under at a face $ n a rights <eos> apple ii february to this british was for their <eos> that the found the year in
48,References
49,"[TIK13] Distributed representations of words and phrases and their compositionality. Proc. Advances in Neural Information Processing Systems 26 31113119, 2013"
50,[Brain] Google deep brain. 
51,[Tensorflow] 
52,[Spacy] 
53,"[DO12] D. Metzler, O. Kurland. Experimental methods for information retrieval. Special Interest Group on Information Retrieval (SIGIR), 2012"
54,[CNLL] 
